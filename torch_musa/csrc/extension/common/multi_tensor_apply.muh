#pragma once

#include <ATen/core/Tensor.h>
#include <c10/macros/Macros.h>
#include <ATen/native/musa/MultiTensorApply.muh>

// Borrowed from cuda/MultiTensorApply.cuh
// Basicly, the kernels defined in extension should avoid relying on the
// functions and data structures defined in pytorch's cu/cuh files as much as
// possible
namespace musa_extension {
namespace {

// may have different settings with CUDA
static constexpr int64_t kILP = 4;
static constexpr int64_t kChunkSize = 65536;
static constexpr int64_t kBlockSize = 512;

static constexpr int depth_to_max_tensors[5] = {110, 64, 48, 36, 30};
static constexpr int depth_to_max_blocks[5] = {320, 320, 320, 320, 320};
static constexpr int depth_to_max_tensors_scalarlist[5] = {96, 64, 48, 36, 30};

// For easy of checking the affect of `__launch_bounds__`
template <typename T, typename U, typename... ArgTypes>
C10_LAUNCH_BOUNDS_1(kBlockSize)
__global__ void multi_tensor_apply_kernel(
    T tensorListMeta,
    U callable,
    ArgTypes... args) {
  // Hand the chunk information to the user-supplied functor to process however
  // it likes.
  callable(kChunkSize, tensorListMeta, args...);
}

using at::native::is_aligned;
using at::native::load_store;
} // namespace

} // namespace musa_extension
