diff --git a/torch/csrc/inductor/aoti_runtime/device_utils.h b/torch/csrc/inductor/aoti_runtime/device_utils.h
index 7b48f49..23e4b4a 100644
--- a/torch/csrc/inductor/aoti_runtime/device_utils.h
+++ b/torch/csrc/inductor/aoti_runtime/device_utils.h
@@ -5,28 +5,28 @@
 // C ABI defined in torch/csrc/inductor/aoti_torch/c/shim.h. The same rule
 // applies to other files under torch/csrc/inductor/aoti_runtime/.
 
-#ifdef USE_CUDA
+#if 1
 
-// FIXME: Currently, CPU and CUDA backend are mutually exclusive.
+// FIXME: Currently, CPU and MUSA backend are mutually exclusive.
 // This is a temporary workaround. We need a better way to support
 // multi devices.
 
-#include <cuda.h>
-#include <cuda_runtime_api.h>
+#include <musa.h>
+#include <musa_runtime_api.h>
 
 #define AOTI_RUNTIME_DEVICE_CHECK(EXPR)                    \
   do {                                                     \
-    const cudaError_t code = EXPR;                         \
-    const char* msg = cudaGetErrorString(code);            \
-    if (code != cudaSuccess) {                             \
+    const musaError_t code = EXPR;                         \
+    const char* msg = musaGetErrorString(code);            \
+    if (code != musaSuccess) {                             \
       throw std::runtime_error(                            \
-          std::string("CUDA error: ") + std::string(msg)); \
+          std::string("MUSA error: ") + std::string(msg)); \
     }                                                      \
   } while (0)
 
 namespace torch::aot_inductor {
 
-using DeviceStreamType = cudaStream_t;
+using DeviceStreamType = musaStream_t;
 
 } // namespace torch::aot_inductor
 
