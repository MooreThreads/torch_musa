diff --git a/aten/src/ATen/native/native_functions.yaml b/aten/src/ATen/native/native_functions.yaml
index 35a1049e209..b15e4d7f601 100644
--- a/aten/src/ATen/native/native_functions.yaml
+++ b/aten/src/ATen/native/native_functions.yaml
@@ -3238,6 +3238,8 @@
   autogen: native_layer_norm_backward.out
   tags: core
 
+- func: rms_norm(Tensor input, int[] normalized_shape, Tensor? weight=None, float? eps=None) -> Tensor
+
 - func: nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor
   variants: function, method
   dispatch:
@@ -14462,6 +14464,31 @@
     CUDA, NestedTensorCUDA: _fused_sdp_choice_cuda
   tags: nondeterministic_seeded

+- func: _scaled_dot_product_attention_math_musa(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> (Tensor output, Tensor attn_weights, Tensor bwd_reserve)
+  variants: function
+  dispatch:
+    CPU: _scaled_dot_product_attention_math_cpu_musa
+  tags: nondeterministic_seeded
+
+- func: _scaled_dot_product_attention_math_musa_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor output, Tensor attn_weights, Tensor bwd_reserve, *, float? scale=None) -> (Tensor grad_query, Tensor grad_key, Tensor grad_value)
+  variants: function
+  dispatch:
+    CPU: _scaled_dot_product_attention_math_cpu_musa_backward
+  tags: nondeterministic_seeded
+
+
+- func: _scaled_dot_product_attention_flash_musa(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> (Tensor output, Tensor logsumexp, Tensor dropout_mask)
+  variants: function
+  dispatch:
+    CPU: _scaled_dot_product_attention_flash_cpu_musa
+  tags: nondeterministic_seeded
+
+- func: _scaled_dot_product_attention_flash_musa_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor output, Tensor logsumexp, Tensor dropout_mask, bool is_causal, Tensor? attn_mask=None, *, float? scale=None) -> (Tensor grad_query, Tensor grad_key, Tensor grad_value)
+  variants: function
+  dispatch:
+    CPU: _scaled_dot_product_attention_flash_cpu_musa_backward
+  tags: nondeterministic_seeded
+
 - func: _scaled_dot_product_attention_math(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, Tensor? dropout_mask=None, *, float? scale=None) -> (Tensor, Tensor)
   variants: function
   tags: nondeterministic_seeded
@@ -15348,3 +15377,16 @@
 # This op is ONLY used by pytorch/XLA in functionalization, and should never show up in vanilla eager mode or in any pytorch tracing contexts.
 - func: _propagate_xla_data(Tensor input, Tensor output) -> ()
   variants: function
+
+- func: gated_silu(Tensor input) -> Tensor
+  variants: function
+
+- func: _fused_rmsnorm_forward(Tensor input, int[] normalized_shape, float eps, Tensor? weight=None) -> (Tensor output, Tensor invvar)
+  variants: function
+  dispatch:
+    CPU: _fused_rmsnorm_forward
+
+- func: _fused_rmsnorm_backward(Tensor grad_out, Tensor invvar, Tensor input, int[] normalized_shape, float eps, Tensor? weight=None) -> (Tensor grad_input, Tensor grad_weight)
+  variants: function
+  dispatch:
+    CPU: _fused_rmsnorm_backward
