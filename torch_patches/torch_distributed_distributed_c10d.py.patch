diff --git a/torch/distributed/distributed_c10d.py b/torch/distributed/distributed_c10d.py
index 547030c..1eb1405 100644
--- a/torch/distributed/distributed_c10d.py
+++ b/torch/distributed/distributed_c10d.py
@@ -89,6 +89,7 @@ __all__ = [
     "is_mpi_available",
     "is_backend_available",
     "is_nccl_available",
+    "is_mccl_available",
     "is_torchelastic_launched",
     "is_ucc_available",
     "is_xccl_available",
@@ -136,6 +137,7 @@ _NCCL_AVAILABLE = True
 _GLOO_AVAILABLE = True
 _UCC_AVAILABLE = True
 _XCCL_AVAILABLE = True
+_MCCL_AVAILABLE = True
 
 _pickler = pickle.Pickler
 _unpickler = pickle.Unpickler
@@ -338,6 +340,8 @@ class Backend(str):  # noqa: SLOT000
             Backend.backend_list.append(name.lower())
 
         if devices is not None:
+            if isinstance(devices, str):
+                devices = [devices]
             for device in devices:
                 if device != "cpu" and device != "cuda":
                     Backend.default_device_backend_map[device] = name.lower()
@@ -1230,6 +1234,9 @@ def is_nccl_available() -> bool:
     """Check if the NCCL backend is available."""
     return _NCCL_AVAILABLE
 
+def is_mccl_available() -> bool:
+    """Check if the MCCL backend is available."""
+    return _MCCL_AVAILABLE
 
 def is_gloo_available() -> bool:
     """Check if the Gloo backend is available."""
@@ -2069,7 +2076,7 @@ def _new_process_group_helper(
     pg._set_group_name(group_name)
     pg._set_group_desc(group_desc)
 
-    if device_id and pg._get_backend(device_id).supports_splitting:
+    if device_id:
         eager_backend = pg._get_backend(device_id)
         eager_backend.eager_connect_single_device(device_id)
 
@@ -2707,6 +2714,10 @@ def broadcast(
         _warn_not_in_group("broadcast")
         return
 
+    if tensor is None or tensor.numel() == 0:
+        warnings.warn(f"input tensor is empty ")
+        return
+
     opts = BroadcastOptions()
     opts.rootRank = group_src
     opts.rootTensor = 0
