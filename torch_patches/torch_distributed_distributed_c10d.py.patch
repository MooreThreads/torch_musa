diff --git a/torch/distributed/distributed_c10d.py b/torch/distributed/distributed_c10d.py
index 27d7919598f..23b47999995 100644
--- a/torch/distributed/distributed_c10d.py
+++ b/torch/distributed/distributed_c10d.py
@@ -50,7 +50,7 @@ __all__ = [
     'gather', 'gather_object', 'get_backend_config', 'get_backend', 'get_rank',
     'get_world_size', 'group', 'init_process_group', 'irecv',
     'is_gloo_available', 'is_initialized', 'is_mpi_available', 'is_backend_available',
-    'is_nccl_available', 'is_torchelastic_launched', 'is_ucc_available',
+    'is_nccl_available', 'is_mccl_available', 'is_torchelastic_launched', 'is_ucc_available',
     'isend', 'monitored_barrier', 'new_group', 'new_subgroups',
     'new_subgroups_by_enumeration', 'recv', 'reduce',
     'reduce_scatter', 'scatter',
@@ -67,6 +67,7 @@ _MPI_AVAILABLE = True
 _NCCL_AVAILABLE = True
 _GLOO_AVAILABLE = True
 _UCC_AVAILABLE = True
+_MCCL_AVAILABLE = True
 
 _pickler = pickle.Pickler
 _unpickler = pickle.Unpickler
@@ -248,6 +249,8 @@ class Backend:
         setattr(Backend, name.upper(), name.lower())
         Backend.backend_list.append(name.lower())
         if devices is not None:
+            if isinstance(devices, str):
+                devices = [devices]
             for device in devices:
                 if device != 'cpu' and device != 'cuda':
                     Backend.default_device_backend_map[device] = name.lower()
@@ -914,6 +917,10 @@ def is_nccl_available() -> bool:
     """Check if the NCCL backend is available."""
     return _NCCL_AVAILABLE
 
+def is_mccl_available() -> bool:
+    """Check if the MCCL backend is available."""
+    return _MCCL_AVAILABLE
+
 
 def is_gloo_available() -> bool:
     """Check if the Gloo backend is available."""
@@ -1461,7 +1468,7 @@ def destroy_process_group(group: Optional[ProcessGroup] = None):
     # alive until all works and hooks are done. The current implementation does the
     # latter. Therefore, we explicitly call _wait_for_pending_works() here to wait
     # for the pending hooks to finish.
-    if pg.name().lower() == "nccl" and pg._has_hooks():
+    if (pg.name().lower() == "nccl" or pg.name().lower() == "mccl") and pg._has_hooks():
         pg._wait_for_pending_works()
 
     if group is None or group == GroupMember.WORLD:
@@ -1858,7 +1865,7 @@ def batch_isend_irecv(p2p_op_list):
     _check_p2p_op_list(p2p_op_list)
     group = p2p_op_list[0].group
     device = p2p_op_list[0].tensor.device
-    if device.type == "cuda":
+    if device.type == "cuda" or device.type == "musa":
         # NCCL style coalescing
         with _coalescing_manager(group, device, async_ops=True) as cm:
             for p2p_op in p2p_op_list:
