diff --git a/torch/nn/parallel/comm.py b/torch/nn/parallel/comm.py
index 764775587d6..20e66ed3bd3 100644
--- a/torch/nn/parallel/comm.py
+++ b/torch/nn/parallel/comm.py
@@ -42,9 +42,9 @@ def broadcast(tensor, devices=None, *, out=None):
         )
     if devices is not None:
         devices = [_get_device_index(d) for d in devices]
-        return torch._C._broadcast(tensor, devices)
+        return torch.musa._MUSAC._broadcast(tensor, devices)
     else:
-        return torch._C._broadcast_out(tensor, out)
+        return torch.musa._MUSAC._broadcast_out(tensor, out)
 
 
 def broadcast_coalesced(tensors, devices, buffer_size=10485760):
@@ -64,7 +64,7 @@ def broadcast_coalesced(tensors, devices, buffer_size=10485760):
     """
     devices = [_get_device_index(d) for d in devices]
     tensors = [_handle_complex(t) for t in tensors]
-    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
+    return torch.musa._MUSAC._broadcast_coalesced(tensors, devices, buffer_size)
 
 
 def reduce_add(inputs, destination=None):
@@ -202,7 +202,7 @@ def scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=
     tensor = _handle_complex(tensor)
     if out is None:
         devices = [_get_device_index(d) for d in devices]
-        return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))
+        return tuple(torch.musa._MUSAC._scatter(tensor, devices, chunk_sizes, dim, streams))
     else:
         if devices is not None:
             raise RuntimeError(
@@ -212,7 +212,7 @@ def scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=
             raise RuntimeError(
                 f"'chunk_sizes' must not be specified when 'out' is specified, but got chunk_sizes={chunk_sizes}"
             )
-        return tuple(torch._C._scatter_out(tensor, out, dim, streams))
+        return tuple(torch.musa._MUSAC._scatter_out(tensor, out, dim, streams))
 
 
 def gather(tensors, dim=0, destination=None, *, out=None):
@@ -251,10 +251,10 @@ def gather(tensors, dim=0, destination=None, *, out=None):
                 stacklevel=2,
             )
         destination = _get_device_index(destination, allow_cpu=True, optional=True)
-        return torch._C._gather(tensors, dim, destination)
+        return torch.musa._MUSAC._gather(tensors, dim, destination)
     else:
         if destination is not None:
             raise RuntimeError(
                 f"'destination' must not be specified when 'out' is specified, but got destination={destination}"
             )
-        return torch._C._gather_out(tensors, out, dim)
+        return torch.musa._MUSAC._gather_out(tensors, out, dim)
