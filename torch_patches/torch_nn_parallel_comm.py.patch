diff --git a/torch/nn/parallel/comm.py b/torch/nn/parallel/comm.py
index 764775587d6..20e66ed3bd3 100644
--- a/torch/nn/parallel/comm.py
+++ b/torch/nn/parallel/comm.py
@@ -32,9 +32,9 @@ def broadcast(tensor, devices=None, *, out=None):
             f"Exactly one of 'devices' and 'out' must be specified, but got devices={devices} and out={out}")
     if devices is not None:
         devices = [_get_device_index(d) for d in devices]
-        return torch._C._broadcast(tensor, devices)
+        return torch.musa._MUSAC._broadcast(tensor, devices)
     else:
-        return torch._C._broadcast_out(tensor, out)
+        return torch.musa._MUSAC._broadcast_out(tensor, out)
 
 
 def broadcast_coalesced(tensors, devices, buffer_size=10485760):
@@ -54,7 +54,7 @@ def broadcast_coalesced(tensors, devices, buffer_size=10485760):
     """
     devices = [_get_device_index(d) for d in devices]
     tensors = [_handle_complex(t) for t in tensors]
-    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
+    return torch.musa._MUSAC._broadcast_coalesced(tensors, devices, buffer_size)
 
 
 def reduce_add(inputs, destination=None):
@@ -184,7 +184,7 @@ def scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=
     tensor = _handle_complex(tensor)
     if out is None:
         devices = [_get_device_index(d) for d in devices]
-        return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))
+        return tuple(torch.musa._MUSAC._scatter(tensor, devices, chunk_sizes, dim, streams))
     else:
         if devices is not None:
             raise RuntimeError(
@@ -192,7 +192,7 @@ def scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=
         if chunk_sizes is not None:
             raise RuntimeError(
                 f"'chunk_sizes' must not be specified when 'out' is specified, but got chunk_sizes={chunk_sizes}")
-        return tuple(torch._C._scatter_out(tensor, out, dim, streams))
+        return tuple(torch.musa._MUSAC._scatter_out(tensor, out, dim, streams))
 
 
 def gather(tensors, dim=0, destination=None, *, out=None):
@@ -228,9 +228,9 @@ def gather(tensors, dim=0, destination=None, *, out=None):
                 'Using -1 to represent CPU tensor is deprecated. Please use a '
                 'device object or string instead, e.g., "cpu".')
         destination = _get_device_index(destination, allow_cpu=True, optional=True)
-        return torch._C._gather(tensors, dim, destination)
+        return torch.musa._MUSAC._gather(tensors, dim, destination)
     else:
         if destination is not None:
             raise RuntimeError(
                 f"'destination' must not be specified when 'out' is specified, but got destination={destination}")
-        return torch._C._gather_out(tensors, out, dim)
+        return torch.musa._MUSAC._gather_out(tensors, out, dim)
