diff --git a/torch/nn/parallel/comm.py b/torch/nn/parallel/comm.py
index 587f88e..2567023 100644
--- a/torch/nn/parallel/comm.py
+++ b/torch/nn/parallel/comm.py
@@ -33,9 +33,9 @@ def broadcast(tensor, devices=None, *, out=None):
             "devices={} and out={}".format(devices, out))
     if devices is not None:
         devices = [_get_device_index(d) for d in devices]
-        return torch._C._broadcast(tensor, devices)
+        return torch.musa._MUSAC._broadcast(tensor, devices)
     else:
-        return torch._C._broadcast_out(tensor, out)
+        return torch.musa._MUSAC._broadcast_out(tensor, out)
 
 
 def broadcast_coalesced(tensors, devices, buffer_size=10485760):
@@ -55,7 +55,7 @@ def broadcast_coalesced(tensors, devices, buffer_size=10485760):
     """
     devices = [_get_device_index(d) for d in devices]
     tensors = [_handle_complex(t) for t in tensors]
-    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
+    return torch.musa._MUSAC._broadcast_coalesced(tensors, devices, buffer_size)
 
 
 def reduce_add(inputs, destination=None):
@@ -186,7 +186,7 @@ def scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=
     tensor = _handle_complex(tensor)
     if out is None:
         devices = [_get_device_index(d) for d in devices]
-        return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))
+        return tuple(torch.musa._MUSAC._scatter(tensor, devices, chunk_sizes, dim, streams))
     else:
         if devices is not None:
             raise RuntimeError(
@@ -196,7 +196,7 @@ def scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=
             raise RuntimeError(
                 "'chunk_sizes' must not be specified when 'out' is specified, "
                 "but got chunk_sizes={}".format(chunk_sizes))
-        return tuple(torch._C._scatter_out(tensor, out, dim, streams))
+        return tuple(torch.musa._MUSAC._scatter_out(tensor, out, dim, streams))
 
 
 def gather(tensors, dim=0, destination=None, *, out=None):
@@ -232,10 +232,10 @@ def gather(tensors, dim=0, destination=None, *, out=None):
                 'Using -1 to represent CPU tensor is deprecated. Please use a '
                 'device object or string instead, e.g., "cpu".')
         destination = _get_device_index(destination, allow_cpu=True, optional=True)
-        return torch._C._gather(tensors, dim, destination)
+        return torch.musa._MUSAC._gather(tensors, dim, destination)
     else:
         if destination is not None:
             raise RuntimeError(
                 "'destination' must not be specified when 'out' is specified, but "
                 "got destination={}".format(destination))
-        return torch._C._gather_out(tensors, out, dim)
+        return torch.musa._MUSAC._gather_out(tensors, out, dim)
