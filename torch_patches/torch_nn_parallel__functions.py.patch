diff --git a/torch/nn/parallel/_functions.py b/torch/nn/parallel/_functions.py
index ef232aa..05c3ef1 100644
--- a/torch/nn/parallel/_functions.py
+++ b/torch/nn/parallel/_functions.py
@@ -101,6 +101,12 @@ class Scatter(Function):
             streams = [
                 _get_stream(torch.device("cuda", device)) for device in target_gpus
             ]
+        privateuse1_backend = torch._C._get_privateuse1_backend_name()
+        privateuse1_mod = getattr(torch, privateuse1_backend)
+        if privateuse1_mod and privateuse1_mod.is_available() and ctx.input_device == -1:
+            streams = [
+                _get_stream(torch.device(privateuse1_backend, device)) for device in target_gpus
+            ]
         outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)
         # Synchronize with the copy stream
         if streams is not None:
