diff --git a/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py b/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py
index e149005..5f4ac1f 100644
--- a/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py
+++ b/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py
@@ -19,6 +19,10 @@ from ._fsdp_collectives import (
     foreach_all_gather,
     foreach_all_gather_copy_out,
     foreach_reduce,
+    AllGather,
+    ReduceScatter,
+    DefaultAllGather,
+    DefaultReduceScatter,
 )
 from ._fsdp_common import (
     compiled_autograd_enabled,
@@ -159,6 +163,9 @@ class FSDPParamGroup:
         self._module_to_pre_save_state_dict_hook_handle: _ModuleToHandleDict = {}
         self._module_to_pre_load_state_dict_hook_handle: _ModuleToHandleDict = {}
         self._all_reduce_hook: Optional[Callable[[torch.Tensor], None]] = None
+        self._forward_all_gather_comm: AllGather = DefaultAllGather()
+        self._backward_all_gather_comm: AllGather = DefaultAllGather()
+        self._reduce_scatter_comm: ReduceScatter = DefaultReduceScatter()
         # Optional stream to run the user-defined all-reduce hook in
         # Saved here and not in the comm. context because we allow the user to
         # specify it, possibly at construction time before lazy init
@@ -264,6 +271,12 @@ class FSDPParamGroup:
             # used in the all-gather streams
             self._wait_all_gather_streams_on_event(self._reshard_after_forward_event)
             self._reshard_after_forward_event = None
+        if self._training_state == TrainingState.FORWARD:
+            all_gather_comm = self._forward_all_gather_comm
+        elif self._training_state == TrainingState.PRE_BACKWARD:
+            all_gather_comm = self._backward_all_gather_comm
+        else:
+            raise RuntimeError(f"Illegal training state: {self._training_state}")
         with record_function(self._with_fqn("FSDP::all_gather")):
             self._all_gather_result = foreach_all_gather(
                 self.fsdp_params,
@@ -271,6 +284,7 @@ class FSDPParamGroup:
                 async_op,
                 *self.comm_ctx.get_all_gather_streams(async_op, self._training_state),
                 self.device,
+                all_gather_comm,
             )
 
     def wait_for_unshard(self):
@@ -444,6 +458,7 @@ class FSDPParamGroup:
                 unsharded_grads,
                 self._reduce_scatter_process_group,
                 self.comm_ctx.reduce_scatter_stream,
+                self._reduce_scatter_comm,
                 self._orig_dtype,
                 self._reduce_dtype,
                 self.device,
