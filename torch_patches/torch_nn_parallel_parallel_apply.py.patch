diff --git a/torch/nn/parallel/parallel_apply.py b/torch/nn/parallel/parallel_apply.py
index eb6c8ec..c8df66e 100644
--- a/torch/nn/parallel/parallel_apply.py
+++ b/torch/nn/parallel/parallel_apply.py
@@ -58,7 +58,7 @@ def parallel_apply(
     else:
         devices = [None] * len(modules)
     devices = [_get_device_index(x, True) for x in devices]
-    streams = [torch.cuda.current_stream(x) for x in devices]
+    streams = [torch.musa.current_stream(x) for x in devices]
     lock = threading.Lock()
     results = {}
     grad_enabled, autocast_enabled = (
@@ -86,11 +86,11 @@ def parallel_apply(
                 return
             device = t.get_device()
         if stream is None:
-            stream = torch.cuda.current_stream(device)
+            stream = torch.musa.current_stream(device)
         try:
-            with torch.cuda.device(device), torch.cuda.stream(
+            with torch.musa.device(device), torch.musa.stream(
                 stream
-            ), torch.amp.autocast("cuda", enabled=autocast_enabled):
+            ), torch.amp.autocast("musa", enabled=autocast_enabled):
                 # this also avoids accidental slicing of `input` if it is a Tensor
                 if not isinstance(input, (list, tuple)):
                     input = (input,)
