diff --git a/aten/src/ATen/native/transformers/attention.cpp b/aten/src/ATen/native/transformers/attention.cpp
index 27397bf..50b2078 100644
--- a/aten/src/ATen/native/transformers/attention.cpp
+++ b/aten/src/ATen/native/transformers/attention.cpp
@@ -35,6 +35,8 @@
 #include <ATen/ops/_scaled_dot_product_attention_math.h>
 #include <ATen/ops/_scaled_dot_product_attention_math_for_mps.h>
 #include <ATen/ops/_scaled_dot_product_attention_math_for_mps_native.h>
+#include <ATen/ops/_scaled_dot_product_attention_flash_musa.h>
+#include <ATen/ops/_scaled_dot_product_attention_math_musa.h>
 #include <ATen/ops/_scaled_dot_product_attention_math_native.h>
 #include <ATen/ops/_scaled_dot_product_efficient_attention.h>
 #include <ATen/ops/_scaled_dot_product_flash_attention.h>
@@ -711,6 +713,38 @@ Tensor scaled_dot_product_attention(
   }
   const auto query_device_type = query_.device().type();
   const auto backend = static_cast<SDPBackend>(choice_int);
+
+  if (query_device_type == DeviceType::PrivateUse1) {
+    switch (backend) {
+      case SDPBackend::flash_attention: {
+        return std::get<0>(at::_scaled_dot_product_attention_flash_musa(
+            query_,
+            key,
+            value,
+            attn_mask_,
+            dropout_p,
+            is_causal,
+            scale));
+      }
+      case SDPBackend::math: {
+        auto [key_expanded, value_expanded] = pre_process_group_query_attention_input(query_, key, value, true);
+        return std::get<0>(at::_scaled_dot_product_attention_math_musa(
+            query_,
+            key_expanded,
+            value_expanded,
+            attn_mask_,
+            dropout_p,
+            is_causal,
+            scale));
+      }
+      default:
+        TORCH_CHECK(
+            false,
+            "No viable backend for scaled_dot_product_attention was found.");
+        return Tensor();
+    }
+  }
+
   const auto convert_attn_func = backend != SDPBackend::cudnn_attention ? convert_boolean_attn_mask : convert_boolean_attn_mask_cudnn;
   auto attn_mask = convert_attn_func(attn_mask_, query_.dtype());
   switch (backend) {
