diff --git a/aten/src/ATen/native/transformers/attention.cpp b/aten/src/ATen/native/transformers/attention.cpp
index 1ebc3e2..c2175ea 100644
--- a/aten/src/ATen/native/transformers/attention.cpp
+++ b/aten/src/ATen/native/transformers/attention.cpp
@@ -35,6 +35,8 @@
 #include <ATen/ops/_scaled_dot_product_attention_math_for_mps.h>
 #include <ATen/ops/_scaled_dot_product_attention_math_for_mps_native.h>
 #include <ATen/ops/_scaled_dot_product_attention_math_native.h>
+#include <ATen/ops/_scaled_dot_product_attention_flash_musa.h>
+#include <ATen/ops/_scaled_dot_product_attention_math_musa.h>
 #include <ATen/ops/_scaled_dot_product_efficient_attention.h>
 #include <ATen/ops/_scaled_dot_product_flash_attention.h>
 #include <ATen/ops/_scaled_dot_product_flash_attention_backward_native.h>
@@ -716,6 +718,38 @@ Tensor scaled_dot_product_attention(
           query_, key, value, attn_mask_, dropout_p, is_causal, scale, enable_gqa);
   }
   sdp::SDPBackend backend = static_cast<sdp::SDPBackend>(choice_int);
+
+  if (query_.is_privateuseone()) {
+    switch (backend) {
+      case sdp::SDPBackend::flash_attention: {
+        return std::get<0>(at::_scaled_dot_product_attention_flash_musa(
+            query_,
+            key,
+            value,
+            attn_mask_,
+            dropout_p,
+            is_causal,
+            scale));
+      }
+      case sdp::SDPBackend::math: {
+        auto [key_expanded, value_expanded] = pre_process_group_query_attention_input(query_, key, value, true);
+        return std::get<0>(at::_scaled_dot_product_attention_math_musa(
+            query_,
+            key_expanded,
+            value_expanded,
+            attn_mask_,
+            dropout_p,
+            is_causal,
+            scale));
+      }
+      default:
+        TORCH_CHECK(
+            false,
+            "No viable backend for scaled_dot_product_attention was found.");
+        return Tensor();
+    }
+  }
+
   switch (backend) {
     case sdp::SDPBackend::cudnn_attention: {
       std::optional<Tensor> attn_mask = convert_boolean_attn_mask_cudnn(attn_mask_, query_.dtype());
