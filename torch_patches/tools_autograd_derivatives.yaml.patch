diff --git a/tools/autograd/derivatives.yaml b/tools/autograd/derivatives.yaml
index 9f7ea3f..8418573 100644
--- a/tools/autograd/derivatives.yaml
+++ b/tools/autograd/derivatives.yaml
@@ -3204,3 +3204,25 @@
 - name: _foreach_norm.Scalar(Tensor[] self, Scalar ord=2, ScalarType? dtype=None) -> Tensor[]
   self: norm_backward(grads[i], self[i], ord, result[i])
   result: norm_jvp(self_p, self_t, ord, result[i])
+
+- name: _fused_rmsnorm_forward(Tensor input, int[] normalized_shape, float eps, Tensor? weight=None) -> (Tensor output, Tensor invvar)
+  output_differentiability: [True, False]
+  input, weight: _fused_rmsnorm_backward(grad, invvar, input, normalized_shape, eps, weight)
+
+- name: _scaled_dot_product_attention_math_musa(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> (Tensor output, Tensor attn_weights, Tensor dropout_mask)
+  output_differentiability: [True, False, False]
+  query, key, value, attn_mask: _scaled_dot_product_attention_math_musa_backward(grad, query, key, value, output, attn_weights, dropout_mask, is_causal, attn_mask, scale)
+
+- name: _scaled_dot_product_attention_flash_musa(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> (Tensor output, Tensor logsumexp, Tensor dropout_mask)
+  output_differentiability: [True, False, False]
+  query, key, value, attn_mask: _scaled_dot_product_attention_flash_musa_backward(grad, query, key, value, output, logsumexp, dropout_mask, is_causal, attn_mask, scale)
+
+- name: _fused_rope_forward(Tensor input, Tensor freq_cis, bool rotary_interleaved, bool batch_first, bool multi_latent_attention=False) -> Tensor
+  input: _fused_rope_backward(grad, freq_cis, rotary_interleaved, batch_first, multi_latent_attention)
+
+- name: _fused_swiglu_forward(Tensor input) -> Tensor
+  output_differentiability: [True]
+  input: _fused_swiglu_backward(grad, input)
+
+- name: _fused_cross_entropy_loss_2d_forward(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, float label_smoothing=0.0) -> Tensor
+  self: _fused_cross_entropy_loss_2d_backward(grad, self, target, weight, reduction, ignore_index, label_smoothing)
