diff --git a/tools/autograd/derivatives.yaml b/tools/autograd/derivatives.yaml
index 1416505..6b93da6 100644
--- a/tools/autograd/derivatives.yaml
+++ b/tools/autograd/derivatives.yaml
@@ -2681,6 +2681,14 @@
   output_differentiability: [True, False, False, False, False]
   query, key, value: _flash_attention_backward(grad, query, key, value, output, softmax_logsumexp, cum_seq_q, cum_seq_k, max_q, max_k, dropout_p, is_causal, philox_seed, philox_offset)
 
+- name: _scaled_dot_product_attention_math_musa(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False) -> (Tensor output, Tensor attn_weights, Tensor bwd_reserve)
+  output_differentiability: [True, False, False]
+  query, key, value: _scaled_dot_product_attention_math_musa_backward(grad, query, key, value, output, attn_weights, bwd_reserve)
+
+- name: _scaled_dot_product_attention_flash_musa(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False) -> (Tensor output, Tensor logsumexp, Tensor dropout_mask)
+  output_differentiability: [True, False, False]
+  query, key, value: _scaled_dot_product_attention_flash_musa_backward(grad, query, key, value, output, logsumexp, dropout_mask, attn_mask)
+
 # fft
 - name: _fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor
   self: fft_r2c_backward(grad, dim, normalization, onesided, self.sym_size(dim.back()))
@@ -2963,3 +2971,7 @@
 - name: _reshape_copy(Tensor self, SymInt[] size) -> Tensor
   self: grad.reshape_symint(self.sym_sizes())
   result: auto_linear
+
+- name: rms_norm_forward(Tensor input, int[] normalized_shape, Tensor? weight=None, float eps=1e-05) -> (Tensor output, Tensor invvar)
+  output_differentiability: [True, False]
+  input, weight: rms_norm_backward(grad, invvar, input, normalized_shape, weight, eps)
