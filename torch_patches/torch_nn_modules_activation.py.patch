diff --git a/torch/nn/modules/activation.py b/torch/nn/modules/activation.py
index 4889a4485c4..6d66ed4555c 100644
--- a/torch/nn/modules/activation.py
+++ b/torch/nn/modules/activation.py
@@ -42,6 +42,8 @@ __all__ = [
     "Softmax",
     "Softmax2d",
     "LogSoftmax",
+    "RoPE",
+    "SwishGLU",
 ]
 
 
@@ -94,6 +96,68 @@ class Threshold(Module):
         return f"threshold={self.threshold}, value={self.value}{inplace_str}"
 
 
+class RoPE(Module):
+    r"""Applies Rotary Positional Embedding function:
+
+    Args:
+        freq_cis: the position tensor to fuse with input
+        batch_first: if True the shape of input is [max_seq_len, batch_size, head_num, dim],
+            otherwise is [batch_size, max_seq_len, head_num, dim], Default: ``False``
+        rotary_interleaved: can rotary interleaved. Default: ``False``
+
+    Shape:
+        - Input: the dim of input must be match (max_seq_len, batch_size, head_num, dim)
+        - Output: :math:`(*)`, same shape as the input.
+    Examples::
+
+        >>> m = nn.RoPE(freq_cis)
+        >>> input = torch.randn(2)
+        >>> output = m(input)
+    """
+
+    rotary_interleaved: Optional[bool]
+    batch_first: Optional[bool]
+    freq_cis: Optional[torch.Tensor]
+
+    def __init__(self, freq_cis: Tensor,
+                rotary_interleaved: Optional[bool]=False,
+                batch_first: Optional[bool]=False) -> None:
+        super().__init__()
+        self.register_buffer("freq_cis", freq_cis)
+        self.rotary_interleaved = rotary_interleaved
+        self.batch_first = batch_first
+
+    def forward(self, input: Tensor) -> Tensor:
+        return F.rope(
+            input=input,
+            freq_cis=self.freq_cis,
+            rotary_interleaved=self.rotary_interleaved,
+            batch_first=self.batch_first
+        )
+
+
+class SwishGLU(Module):
+    r"""Applies the Swish Gated Linear Unit (SwishGLU) function, element-wise.
+    The SwishGLU function is also known as the swish_glu function.
+
+    .. math::
+        \text{SwishGLU}(x) = Swish(x[0]) * \(x[1]), \text{where } \Swish(x) \text{ is silu.}
+
+    .. note::
+        See `GLU with Swish <https://arxiv.org/pdf/2002.05202v1>`
+
+    Shape:
+        - Input: must be 2 dim, and the last dimension must be an integer multiple of 2
+    Examples::
+
+        >>> m = nn.SwishGLU()
+        >>> input = torch.randn(2, 4)
+        >>> output = m(input)
+    """
+    def forward(self, input: Tensor) -> Tensor:
+        return F.swish_glu(input)
+
+
 class ReLU(Module):
     r"""Applies the rectified linear unit function element-wise.
 
