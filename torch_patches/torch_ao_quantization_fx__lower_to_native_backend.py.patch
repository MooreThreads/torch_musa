diff --git a/torch/ao/quantization/fx/_lower_to_native_backend.py b/torch/ao/quantization/fx/_lower_to_native_backend.py
index e769b6c5265..e8f67efa627 100644
--- a/torch/ao/quantization/fx/_lower_to_native_backend.py
+++ b/torch/ao/quantization/fx/_lower_to_native_backend.py
@@ -110,7 +110,7 @@ def is_copy_node(node, modules):
         torch._C._nn.avg_pool3d,
         torch.clamp,
         torch.flatten,
-        torch.mean,
+        # torch.mean,
         operator.floordiv,
         # F.channel_shuffle and torch.channel_shuffle are essentially the same thing
         # so we only need to put one of them here
@@ -118,7 +118,7 @@ def is_copy_node(node, modules):
     ]
     method_list = [
         "clamp",
-        "mean",
+        # "mean",
         "relu",
         "relu_",
     ]
@@ -273,6 +273,7 @@ STATIC_LOWER_FUSED_MODULE_MAP: Dict[Type[nn.Module], Tuple[Type[nn.Module], Type
     nni.ConvReLU1d: (nnqr.Conv1d, nniq.ConvReLU1d),
     nni.ConvReLU2d: (nnqr.Conv2d, nniq.ConvReLU2d),
     nni.ConvReLU3d: (nnqr.Conv3d, nniq.ConvReLU3d),
+    nni.ConvSiLU2d: (nnqr.Conv2d, nniq.ConvSiLU2d),
 }
 
 # The difference between STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP and STATIC_LOWER_FUSED_MODULE_MAP:
@@ -283,6 +284,7 @@ STATIC_LOWER_FUSED_MODULE_MAP: Dict[Type[nn.Module], Tuple[Type[nn.Module], Type
 STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP: Dict[Type[nn.Module], Tuple[Type[nn.Module], Type[WeightedQuantizedModule]]] = {
     nni.ConvAdd2d: (nnqr.Conv2d, nniq.ConvAdd2d),
     nni.ConvAddReLU2d: (nnqr.Conv2d, nniq.ConvAddReLU2d),
+    nni.ConvAddSiLU2d: (nnqr.Conv2d, nniq.ConvAddSiLU2d),
 }
 
 # Mapping from fused module class to a 2-tuple of:
@@ -1002,7 +1004,8 @@ def _lower_quantized_binary_op(
         model.graph.erase_node(q_node)
         if relu_node is not None:
             model.graph.erase_node(relu_node)
-        model.graph.erase_node(bop_node)
+        if not bop_node.users:
+            model.graph.erase_node(bop_node)
 
 def special_pattern_replacement(model: GraphModule):
     modules = dict(model.named_modules(remove_duplicate=False))
