diff --git a/torch/nn/functional.py b/torch/nn/functional.py
index 3072ac0fef0..01861bde8bd 100644
--- a/torch/nn/functional.py
+++ b/torch/nn/functional.py
@@ -1742,6 +1742,18 @@ def glu(input: Tensor, dim: int = -1) -> Tensor:  # noqa: D400,D402
     return torch._C._nn.glu(input, dim)
 
 
+def swish_glu(input: Tensor) -> Tensor:  # noqa: D400,D402
+    r"""swish_glu(input, inplace=False) -> Tensor
+
+    Applies swish gate linear unit operator.
+    See :class:`~torch.nn.SwiGLU` for more details.
+    """
+    if has_torch_function_unary(input):
+        return handle_torch_function(swish_glu, (input,), input)
+    result = torch.swish_glu(input)
+    return result
+
+
 def hardtanh(
     input: Tensor,
     min_val: float = -1.0,
@@ -2957,6 +2969,21 @@ def group_norm(
     )
 
 
+def rope(
+    input: Tensor,
+    freq_cis: Tensor,
+    rotary_interleaved: bool,
+    batch_first: bool
+) -> Tensor:
+    r"""Rotary position embedding  forward""
+    """
+    if has_torch_function_variadic(input, freq_cis):
+        return handle_torch_function(
+            rope, (input, freq_cis), input, rotary_interleaved, batch_first
+        )
+    return torch.rope(input, freq_cis, rotary_interleaved, batch_first)
+
+
 def local_response_norm(
     input: Tensor,
     size: int,
