diff --git a/aten/src/ATen/native/musa_unique.cpp b/aten/src/ATen/native/musa_unique.cpp
new file mode 100644
index 0000000..0b936fb
--- /dev/null
+++ b/aten/src/ATen/native/musa_unique.cpp
@@ -0,0 +1,160 @@
+#include <chrono>
+#include <iostream>
+#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
+#include <ATen/Dispatch.h>
+#include <ATen/Parallel.h>
+#include <ATen/core/Tensor.h>
+#include <ATen/native/cpu/mixed_data_type.h>
+#include <c10/util/irange.h>
+
+#ifndef AT_PER_OPERATOR_HEADERS
+#include <ATen/Functions.h>
+#include <ATen/NativeFunctions.h>
+#else
+#include <ATen/ops/cross_entropy_loss_2d_choice_native.h>
+#include <ATen/ops/_fused_cross_entropy_loss_2d_forward_native.h>
+#include <ATen/ops/_fused_cross_entropy_loss_2d_backward_native.h>
+#include <ATen/ops/_fused_rmsnorm_backward_native.h>
+#include <ATen/ops/_fused_rmsnorm_forward_native.h>
+#include <ATen/ops/_fused_rope_backward_native.h>
+#include <ATen/ops/_fused_rope_forward.h>
+#include <ATen/ops/_fused_rope_forward_native.h>
+#include <ATen/ops/_fused_swiglu_backward_native.h>
+#include <ATen/ops/_fused_swiglu_forward.h>
+#include <ATen/ops/_fused_swiglu_forward_native.h>
+#include <ATen/ops/gated_silu_native.h>
+#endif
+
+namespace at::native {
+
+namespace {
+
+[[noreturn]] inline void NYI(const char* op_name) {
+  TORCH_CHECK(false, op_name, " only supported in torch_musa.");
+}
+
+} // anonymous namespace
+
+// ------- gated silu --------
+Tensor gated_silu(const Tensor& input) {
+  NYI("gated_silu");
+}
+
+// ------- rms_norm -------
+std::tuple<Tensor, Tensor> _fused_rmsnorm_forward(
+    const Tensor& input,
+    IntArrayRef normalized_shape,
+    double eps,
+    const c10::optional<Tensor>& weight) {
+  NYI("_fused_rmsnorm_forward");
+}
+
+std::tuple<Tensor, Tensor> _fused_rmsnorm_backward(
+    const Tensor& grad_out,
+    const Tensor& invvar,
+    const Tensor& input,
+    IntArrayRef normalized_shape,
+    double eps,
+    const c10::optional<Tensor>& weight) {
+  NYI("_fused_rmsnorm_backward");
+}
+
+// ------- RoPE -------
+Tensor rope(
+    const Tensor& input,
+    const Tensor& freq_cis,
+    bool rotary_interleaved,
+    bool batch_first) {
+  auto backend = input.device().type();
+  if (backend == DeviceType::PrivateUse1) {
+    // This is for musa fused Rope
+    auto result = AT_DISPATCH_FLOATING_TYPES_AND2(
+        at::ScalarType::Half,
+        at::ScalarType::BFloat16,
+        input.scalar_type(),
+        "rope",
+        [&] {
+          auto output = at::_fused_rope_forward(
+              input, freq_cis, rotary_interleaved, batch_first);
+          return output;
+        });
+    return result;
+  }
+  NYI("rope");
+}
+
+Tensor _fused_rope_forward(
+    const Tensor& input,
+    const Tensor& freq_cis,
+    bool rotary_interleaved,
+    bool batch_first) {
+  NYI("_fused_rope_forward");
+}
+
+Tensor _fused_rope_backward(
+    const Tensor& grad,
+    const Tensor& freq_cis,
+    bool rotary_interleaved,
+    bool batch_first) {
+  NYI("_fused_rope_backward");
+}
+
+// --------- SwiGLU ----------
+Tensor swish_glu(const Tensor& input) {
+  auto backend = input.device().type();
+  if (backend == DeviceType::PrivateUse1) {
+    // This is for musa fused swish glu
+    auto result = AT_DISPATCH_FLOATING_TYPES_AND2(
+        at::ScalarType::Half,
+        at::ScalarType::BFloat16,
+        input.scalar_type(),
+        "swish_glu",
+        [&] {
+          auto output = at::_fused_swiglu_forward(input);
+          return output;
+        });
+    return result;
+  }
+  NYI("swish_glu");
+}
+
+Tensor _fused_swiglu_forward(const Tensor& input) {
+  NYI("_fused_swiglu_forward");
+}
+
+Tensor _fused_swiglu_backward(const Tensor& grad, const Tensor& input) {
+  NYI("_fused_swiglu_backward");
+}
+
+int64_t cross_entropy_loss_2d_choice_cpu(
+    const Tensor& self,
+    const Tensor& target,
+    const c10::optional<Tensor>& weight,
+    int64_t reduction,
+    int64_t ignore_index,
+    double label_smoothing) {
+  return 0;
+}
+
+Tensor _fused_cross_entropy_loss_2d_forward_cpu(
+    const Tensor& self,
+    const Tensor& target,
+    const c10::optional<Tensor>& weight,
+    int64_t reduction,
+    int64_t ignore_index,
+    double label_smoothing) {
+  NYI("_fused_cross_entropy_loss_2d_forward");
+}
+
+Tensor _fused_cross_entropy_loss_2d_backward_cpu(
+    const Tensor& grad_output,
+    const Tensor& self,
+    const Tensor& target,
+    const c10::optional<Tensor>& weight,
+    int64_t reduction,
+    int64_t ignore_index,
+    double label_smoothing) {
+  NYI("_fused_cross_entropy_loss_2d_backward");
+}
+
+} // namespace at::native
