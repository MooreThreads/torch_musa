diff --git a/aten/src/ATen/native/musa_unique.cpp b/aten/src/ATen/native/musa_unique.cpp
new file mode 100644
index 0000000..1f0c0da
--- /dev/null
+++ b/aten/src/ATen/native/musa_unique.cpp
@@ -0,0 +1,197 @@
+#include <ATen/Dispatch.h>
+
+#ifndef AT_PER_OPERATOR_HEADERS
+#include <ATen/Functions.h>
+#include <ATen/NativeFunctions.h>
+#else
+#include <ATen/ops/gated_silu_native.h>
+#include <ATen/ops/cross_entropy_loss_2d_choice_native.h>
+#include <ATen/ops/_fused_cross_entropy_loss_2d_forward_native.h>
+#include <ATen/ops/_fused_cross_entropy_loss_2d_backward_native.h>
+#include <ATen/ops/_fused_rmsnorm_forward_native.h>
+#include <ATen/ops/_fused_rmsnorm_backward_native.h>
+#include <ATen/ops/_fused_rope_backward_native.h>
+#include <ATen/ops/_fused_rope_forward.h>
+#include <ATen/ops/_fused_rope_forward_native.h>
+#include <ATen/ops/_fused_swiglu_backward_native.h>
+#include <ATen/ops/_fused_swiglu_forward.h>
+#include <ATen/ops/_fused_swiglu_forward_native.h>
+#include <ATen/ops/_scaled_dot_product_attention_math_musa_native.h>
+#include <ATen/ops/_scaled_dot_product_attention_math_musa_backward_native.h>
+#include <ATen/ops/_scaled_dot_product_attention_flash_musa_native.h>
+#include <ATen/ops/_scaled_dot_product_attention_flash_musa_backward_native.h>
+#endif
+
+namespace at::native {
+
+namespace {
+
+[[noreturn]] inline void NYI(const char* op_name) {
+  TORCH_CHECK(false, op_name, " only supported in torch_musa.");
+}
+
+} // anonymous namespace
+
+Tensor gated_silu(const Tensor& input) {
+  NYI("gated_silu");
+}
+
+std::tuple<Tensor, Tensor> _fused_rmsnorm_forward(
+    const Tensor& input,
+    IntArrayRef normalized_shape,
+    double eps,
+    const std::optional<Tensor>& weight) {
+  NYI("_fused_rmsnorm_forward");
+}
+
+std::tuple<Tensor, Tensor> _fused_rmsnorm_backward(
+    const Tensor& grad_out,
+    const Tensor& invvar,
+    const Tensor& input,
+    IntArrayRef normalized_shape,
+    double eps,
+    const std::optional<Tensor>& weight) {
+  NYI("_fused_rmsnorm_backward");
+}
+
+std::tuple<Tensor, Tensor, Tensor>
+_scaled_dot_product_attention_math_cpu_musa(
+    const Tensor& query,
+    const Tensor& key,
+    const Tensor& value,
+    const std::optional<Tensor>& attn_mask,
+    double dropout_p,
+    bool is_causal,
+    std::optional<double> scale) {
+  NYI("_scaled_dot_product_attention_math_musa");
+}
+
+std::tuple<Tensor, Tensor, Tensor>
+_scaled_dot_product_attention_math_cpu_musa_backward(
+    const Tensor& grad_out,
+    const Tensor& query,
+    const Tensor& key,
+    const Tensor& value,
+    const Tensor& output,
+    const Tensor& atten_probs,
+    const Tensor& dropout_mask,
+    std::optional<double> scale) {
+  NYI("_scaled_dot_product_attention_math_musa_backward");
+}
+
+std::tuple<Tensor, Tensor, Tensor>
+_scaled_dot_product_attention_flash_cpu_musa(
+    const Tensor& query,
+    const Tensor& key,
+    const Tensor& value,
+    const std::optional<Tensor>& attn_mask,
+    double dropout_p,
+    bool is_causal,
+    std::optional<double> scale) {
+  NYI("_scaled_dot_product_attention_flash_musa");
+}
+
+std::tuple<Tensor, Tensor, Tensor>
+_scaled_dot_product_attention_flash_cpu_musa_backward(
+    const Tensor& grad_out,
+    const Tensor& query,
+    const Tensor& key,
+    const Tensor& value,
+    const Tensor& output,
+    const Tensor& logsumexp,
+    const Tensor& dropout_mask,
+    bool is_causal,
+    const std::optional<Tensor>& attn_mask,
+    std::optional<double> scale) {
+  NYI("_scaled_dot_product_attention_flash_musa_backward");
+}
+
+Tensor _fused_rope_forward(
+    const Tensor& input,
+    const Tensor& freq_cis,
+    bool rotary_interleaved,
+    bool batch_first,
+    bool multi_latent_attention) {
+  NYI("_fused_rope_forward");
+}
+
+Tensor _fused_rope_backward(
+    const Tensor& grad,
+    const Tensor& freq_cis,
+    bool rotary_interleaved,
+    bool batch_first,
+    bool multi_latent_attention) {
+  NYI("_fused_rope_backward");
+}
+
+Tensor rope(const Tensor& input, const Tensor& freq_cls, bool rotary_interleaved, bool batch_first, bool multi_latent_attention) {
+  if (input.is_privateuseone()) {
+    auto result = AT_DISPATCH_FLOATING_TYPES_AND2(
+        at::ScalarType::Half,
+        at::ScalarType::BFloat16,
+        input.scalar_type(),
+        "rope",[&]{
+            auto output = at::_fused_rope_forward(input, freq_cls, rotary_interleaved, batch_first, multi_latent_attention);
+            return output;
+        });
+    return result;
+  }
+  NYI("rope");
+}
+
+Tensor _fused_swiglu_forward(const Tensor& input) {
+  NYI("_fused_swiglu_forward");
+}
+
+Tensor _fused_swiglu_backward(const Tensor& grad, const Tensor& input) {
+  NYI("_fused_swiglu_backward");
+}
+
+Tensor swish_glu(const Tensor& input) {
+  if (input.is_privateuseone()) {
+    auto result = AT_DISPATCH_FLOATING_TYPES_AND2(
+        at::ScalarType::Half,
+        at::ScalarType::BFloat16,
+        input.scalar_type(),
+        "swish_glu",
+        [&] {
+          auto output = at::_fused_swiglu_forward(input);
+          return output;
+        });
+    return result;
+  }
+  NYI("swish_glu");
+}
+
+int64_t cross_entropy_loss_2d_choice_cpu(
+    const Tensor& self,
+    const Tensor& target,
+    const std::optional<Tensor>& weight,
+    int64_t reduction,
+    int64_t ignore_index,
+    double label_smoothing) {
+  return 0;
+}
+
+Tensor _fused_cross_entropy_loss_2d_forward_cpu(
+    const Tensor& self,
+    const Tensor& target,
+    const std::optional<Tensor>& weight,
+    int64_t reduction,
+    int64_t ignore_index,
+    double label_smoothing) {
+  NYI("_fused_cross_entropy_loss_2d_forward");
+}
+
+Tensor _fused_cross_entropy_loss_2d_backward_cpu(
+    const Tensor& grad_output,
+    const Tensor& self,
+    const Tensor& target,
+    const std::optional<Tensor>& weight,
+    int64_t reduction,
+    int64_t ignore_index,
+    double label_smoothing) {
+  NYI("_fused_cross_entropy_loss_2d_backward");
+}
+
+} // namespace at::native
