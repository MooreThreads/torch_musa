From 8774e2e10874e92dfcab0ec68301c50ed1f27ada Mon Sep 17 00:00:00 2001
From: "zhi.cai@mthreads.com" <zhi.cai@mthreads.com>
Date: Mon, 12 Jun 2023 10:09:02 +0800
Subject: [PATCH] feat(op): support PrivateUse1 in is_fused_kernel_accaptable

---
 aten/src/ATen/native/Dropout.cpp | 7 ++++++-
 1 file changed, 6 insertions(+), 1 deletion(-)

diff --git a/aten/src/ATen/native/Dropout.cpp b/aten/src/ATen/native/Dropout.cpp
index 2903fac..1561949 100644
--- a/aten/src/ATen/native/Dropout.cpp
+++ b/aten/src/ATen/native/Dropout.cpp
@@ -42,8 +42,13 @@ Tensor make_feature_noise(const Tensor& input) {
   return input.new_empty_symint(sizes);
 }
 
+// Note: if "is_fused_kernel_acceptable" is true, then dropout op will be implemented
+// by native_dropout function, otherwise dropout op will be implemented by combining
+// several operators, including bernoulli_, multiply, div, mul, which will result in
+// bad performance. The new hardware backend should act as CUDA, so here PrivateUse1
+// is supported in "is_fused_kernel_acceptable".
 bool is_fused_kernel_acceptable(const Tensor& input, double p) {
-  return (input.is_cuda() || input.is_xpu() || input.is_lazy()) && p > 0 && p < 1 && input.sym_numel() > 0;
+  return (input.is_cuda() || input.is_xpu() || input.is_lazy() || input.device().type() == kPrivateUse1) && p > 0 && p < 1 && input.sym_numel() > 0;
 }
 
 // NB: sure, we could have used different overloads here, but I would feel insecure
-- 
2.25.1

