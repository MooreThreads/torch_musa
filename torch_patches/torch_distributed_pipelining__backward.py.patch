diff --git a/torch/distributed/pipelining/_backward.py b/torch/distributed/pipelining/_backward.py
index a4c5516..a7c2f38 100644
--- a/torch/distributed/pipelining/_backward.py
+++ b/torch/distributed/pipelining/_backward.py
@@ -286,7 +286,7 @@ def stage_backward(
         stage_output_tensors = []
         output_grad_tensors = []
 
-        def extract_tensors_with_grads(output_val, grad_val):
+        def extract_tensors_with_grads(output_val, grad_val, extract_tensors_with_grads):
             if isinstance(output_val, torch.Tensor):
                 if not output_val.requires_grad and output_val.grad_fn is None:
                     return
@@ -303,19 +303,28 @@ def stage_backward(
                 ), f"grad_value expected to have type {type(output_val)} but got {type(grad_val)}"
                 assert len(output_val) == len(grad_val)
                 for ov, gv in zip(output_val, grad_val):
-                    extract_tensors_with_grads(ov, gv)
+                    extract_tensors_with_grads(ov, gv, extract_tensors_with_grads)
             elif isinstance(output_val, dict):
                 if grad_val is None:
                     return
                 assert isinstance(grad_val, dict)
                 assert set(output_val.keys()) == set(grad_val.keys())
                 for k in output_val.keys():
-                    extract_tensors_with_grads(output_val[k], grad_val[k])
+                    extract_tensors_with_grads(output_val[k], grad_val[k], extract_tensors_with_grads)
             else:
                 # Output is a non-tensor type; just ignore it
                 pass
-
-        extract_tensors_with_grads(stage_output, output_grads)
+        
+        # Note(mt-ai-infra): This ref cycle issue was fixed since PT2.6, we can remove this patch since then
+        # Note: ref cycle
+        # break a ref cycle that would keep tensors alive until GC runs
+        # 1. extract_tensors_with_grads refers to a cell that holds refs to any vars defined in stage_backward
+        #    and used in extract_tensors_with_grads
+        # 2. extract_tensors_with_grads referred to both stage_output_tensors, output_grad_tensors,
+        #    and to itself (extract_tensors_with_grads) since it makes a recursive call
+        # 3. stage_output_tensors was kept alive by the above refcycle, and it holds activation tensors, which is bad
+        # fix -> explictly pass in the ref to the fn, so there is no gc cycle anymore
+        extract_tensors_with_grads(stage_output, output_grads, extract_tensors_with_grads)
 
         torch.autograd.backward(
             stage_output_tensors, grad_tensors=output_grad_tensors  # type: ignore[arg-type]
