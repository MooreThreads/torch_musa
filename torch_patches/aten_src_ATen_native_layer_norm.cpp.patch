diff --git a/aten/src/ATen/native/layer_norm.cpp b/aten/src/ATen/native/layer_norm.cpp
index 0972978..c20182c 100644
--- a/aten/src/ATen/native/layer_norm.cpp
+++ b/aten/src/ATen/native/layer_norm.cpp
@@ -15,6 +15,7 @@
 #include <ATen/ops/empty.h>
 #include <ATen/ops/empty_like.h>
 #include <ATen/ops/empty_like_native.h>
+#include <ATen/ops/_fused_rmsnorm_forward.h>
 #include <ATen/ops/layer_norm_native.h>
 #include <ATen/ops/native_batch_norm.h>
 #include <ATen/ops/native_layer_norm.h>
@@ -286,6 +287,21 @@ Tensor rms_norm_symint(
   }
 #endif
 
+  if (input.is_privateuseone()) {
+    auto result = AT_DISPATCH_FLOATING_TYPES_AND2(
+        at::ScalarType::Half,
+        at::ScalarType::BFloat16,
+        input.scalar_type(),
+        "rms_norm",
+        [&] {
+      using limits = std::numeric_limits<double>;
+      double eps_val = eps.value_or(limits::epsilon());
+      auto output_and_invvar = at::_fused_rmsnorm_forward_symint(input, normalized_shape, eps_val, weight_opt);
+      return std::get<0>(output_and_invvar);
+    });
+    return result;
+  }
+
   std::vector<int64_t> dims_to_reduce;
   for (const auto i : c10::irange(normalized_shape.size())) {
     dims_to_reduce.push_back(input.dim() - i - 1);
