diff --git a/aten/src/ATen/native/layer_norm.cpp b/aten/src/ATen/native/layer_norm.cpp
index 7b5f5e1d939..fb1e8773fdc 100644
--- a/aten/src/ATen/native/layer_norm.cpp
+++ b/aten/src/ATen/native/layer_norm.cpp
@@ -2,6 +2,7 @@
 #include <ATen/native/layer_norm.h>
 
 #include <ATen/core/Tensor.h>
+#include <ATen/Dispatch.h>
 #include <ATen/Parallel.h>
 #include <ATen/native/cpu/mixed_data_type.h>
 #include <c10/util/irange.h>
@@ -13,11 +14,15 @@
 #include <ATen/ops/empty.h>
 #include <ATen/ops/empty_like.h>
 #include <ATen/ops/empty_like_native.h>
+#include <ATen/ops/_fused_rmsnorm_forward.h>
 #include <ATen/ops/layer_norm_native.h>
 #include <ATen/ops/native_batch_norm.h>
 #include <ATen/ops/native_layer_norm.h>
 #include <ATen/ops/native_layer_norm_backward_native.h>
 #include <ATen/ops/native_layer_norm_native.h>
+#include <ATen/ops/pow.h>
+#include <ATen/ops/rsqrt.h>
+#include <ATen/ops/rms_norm.h>
 #include <ATen/ops/zeros_like_native.h>
 #endif
 
@@ -259,5 +264,75 @@ std::tuple<Tensor, Tensor, Tensor> math_native_layer_norm(
   rstd = rstd.view(stat_shape);
   return std::make_tuple(out, mean, rstd);
 }
+
+
+Tensor rms_norm(
+    const Tensor& input,
+    IntArrayRef normalized_shape,
+    const c10::optional<Tensor>& weight_opt /* optional */,
+    c10::optional<double> eps) {
+
+  auto backend = input.device().type();
+
+  if(backend == DeviceType::PrivateUse1){
+    // This is for musa fused RMSNorm
+     auto result = AT_DISPATCH_FLOATING_TYPES_AND2(
+        at::ScalarType::Half,
+        at::ScalarType::BFloat16,
+        input.scalar_type(),
+        "rms_norm",[&]{
+            double eps_val;
+            if (!eps.has_value()) {
+                eps_val = std::numeric_limits<at::scalar_value_type<double>::type>::epsilon();
+            } else {
+                eps_val = eps.value();
+            }
+
+            auto outputs_and_invvar =
+                at::_fused_rmsnorm_forward(input, normalized_shape, eps_val, weight_opt);
+            return std::get<0>(outputs_and_invvar);
+        });
+    return result;
+  } else {
+  // See [Note: hacky wrapper removal for optional tensor]
+    c10::MaybeOwned<Tensor> weight_maybe_owned = at::borrow_from_optional_tensor(weight_opt);
+    const Tensor& weight = *weight_maybe_owned;
+    auto bias_opt = at::optional<Tensor>();
+    const Tensor& bias = *at::borrow_from_optional_tensor(bias_opt);
+    (void) _check_layer_norm_inputs(input, normalized_shape, weight, bias);
+
+    std::vector<int64_t> dims_to_reduce;
+    for (const auto i : c10::irange(normalized_shape.size())) {
+      dims_to_reduce.push_back(input.dim() - i - 1);
+    }
+    IntArrayRef dims_to_reduce_ref = IntArrayRef(dims_to_reduce);
+
+    auto result = AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(
+          at::ScalarType::Half,
+          at::ScalarType::BFloat16,
+          input.scalar_type(),
+          "rms_norm",
+          [&] {
+      scalar_t eps_val;
+      if (!eps.has_value()) {
+        eps_val = std::numeric_limits<at::scalar_value_type<scalar_t>::type>::epsilon();
+      } else {
+        eps_val = eps.value();
+      }
+
+      auto result = input.mul(at::rsqrt(at::pow(input, 2).mean(dims_to_reduce_ref, /*keep_dim=*/true).add_(eps_val)));
+
+      if (weight_opt.has_value()) {
+        result = result.mul(weight_opt.value());
+      }
+
+      return result;
+    });
+
+    return result;
+  }
+}
+
+
 } // namespace native
 } // namespace at
