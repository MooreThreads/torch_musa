diff --git a/aten/src/ATen/native/layer_norm.cpp b/aten/src/ATen/native/layer_norm.cpp
index b11bcab..42855a3 100644
--- a/aten/src/ATen/native/layer_norm.cpp
+++ b/aten/src/ATen/native/layer_norm.cpp
@@ -14,6 +14,7 @@
 #include <ATen/ops/empty.h>
 #include <ATen/ops/empty_like.h>
 #include <ATen/ops/empty_like_native.h>
+#include <ATen/ops/_fused_rmsnorm_forward.h>
 #include <ATen/ops/layer_norm_native.h>
 #include <ATen/ops/native_batch_norm.h>
 #include <ATen/ops/native_layer_norm.h>
@@ -276,6 +277,26 @@ Tensor rms_norm(
   const Tensor& bias = *at::borrow_from_optional_tensor(bias_opt);
   (void) _check_layer_norm_inputs(input, normalized_shape, weight, bias);
 
+  if (input.is_privateuseone()) {
+    auto result = AT_DISPATCH_FLOATING_TYPES_AND2(
+        at::ScalarType::Half,
+        at::ScalarType::BFloat16,
+        input.scalar_type(),
+        "rms_norm",
+        [&] {
+      double eps_val;
+      if (!eps.has_value()) {
+          eps_val = std::numeric_limits<at::scalar_value_type<double>::type>::epsilon();
+      } else {
+          eps_val = eps.value();
+      }
+      auto outputs_and_invvar =
+          at::_fused_rmsnorm_forward(input, normalized_shape, eps_val, weight_opt);
+      return std::get<0>(outputs_and_invvar);
+    });
+    return result;
+  }
+
   std::vector<int64_t> dims_to_reduce;
   for (const auto i : c10::irange(normalized_shape.size())) {
     dims_to_reduce.push_back(input.dim() - i - 1);
